{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe34c2c-0a02-4991-a579-f6fadf5955ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0797ee-e03f-462c-9f8b-eee35889babe",
   "metadata": {},
   "source": [
    "FUNZIONI UTILI PER L'ANALISI DEL TESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31130c55-85df-445c-81cb-af7cb83802cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un dizionario, in cui ad ogni parola (chiave) corrisponde una lista\n",
    "# di babel synset presi del file \"SemEval17_IT_senses2sysensts.txt\"\n",
    "def get_senses_dictionary(word_list):\n",
    "    senses_for_words = dict()\n",
    "    file = open(\"SemEval17_IT_senses2synsets.txt\", \"r\", encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, len(lines)):\n",
    "        line = lines[i]\n",
    "        line = line.replace(\"\\n\", \"\").replace(\"#\", \"\")\n",
    "        if line in word_list:\n",
    "            while True:\n",
    "                i += 1\n",
    "                babel_synset = lines[i].replace(\"\\n\", \"\")\n",
    "                if \"#\" in babel_synset:  # vuol dire che non sto considerando più un babel synset\n",
    "                    break\n",
    "                if line not in senses_for_words:\n",
    "                    senses_for_words[line] = [babel_synset]\n",
    "                else:\n",
    "                    senses_for_words[line].append(babel_synset)\n",
    "    return senses_for_words\n",
    "\n",
    "# crea un dizionario delle annotazioni, in cui ad ogni coppia di parole\n",
    "# prese dal file delle annotazioni \"annotations1.tsv\" associa il valore\n",
    "# di similarità annotato da un essere umano\n",
    "def get_human_similarities_dictionary():\n",
    "    human_similarities = dict()\n",
    "    annotations_file = open(\"annotations1.tsv\", encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(annotations_file, delimiter=\"\\t\")\n",
    "    for row in read_tsv:\n",
    "        if row[0]:  # verifico che non sia una riga vuota (può capitare)\n",
    "            human_similarities[(row[0], row[1])] = row[2]\n",
    "    annotations_file.close()\n",
    "    return human_similarities\n",
    "\n",
    "# crea un dizionario delle annotazioni, in cui ad ogni coppia di parola\n",
    "# prese dal file delle annotazioni \"annotations2.tsv\" associa una coppia\n",
    "# di BABEL synset annotati da un essere umano sulla base delle annotazioni\n",
    "# di similarità del file \"annotations1.tsv\"\n",
    "def get_human_synsets_dictionary():\n",
    "    human_synsets = dict()\n",
    "    annotations_file = open(\"annotations2.tsv\", encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(annotations_file, delimiter=\"\\t\")\n",
    "    for row in read_tsv:\n",
    "        if row[0]:  # verifico che non sia una riga vuota (può capitare)\n",
    "            human_synsets[(row[0], row[1])] = (row[2], row[3])\n",
    "    annotations_file.close()\n",
    "    return human_synsets\n",
    "\n",
    "# restituisce tutte le parole presenti nella coppie valutate nel dizionario\n",
    "# delle annotazioni umane che viene dato in input\n",
    "def get_word_list(human_similarities_dictionary):\n",
    "    word_list = []\n",
    "    for pair in human_similarities_dictionary.keys():\n",
    "        word_list.append(pair[0])\n",
    "        word_list.append(pair[1])\n",
    "    return word_list\n",
    "\n",
    "# crea una dizionario che associa ad ogni coppia di parole del dizionario\n",
    "# delle annotazioni umane delle similarità, un valore di similarità dato dalla massimizzazione della similarità\n",
    "# del coseno tra l'insieme dei vettori nasari associati ad una parola\n",
    "# e l'insieme dei vettori nasari associati all'altra parola\n",
    "# CONSEGNA 1\n",
    "def get_NASARI_similarities_dictionary(human_similarities_dictionary, senses_dictionary):\n",
    "    similarity_dictionary = dict()\n",
    "    for word_pair in human_similarities_dictionary:\n",
    "\n",
    "        try:\n",
    "            word1_senses = senses_dictionary[word_pair[0]]\n",
    "            word2_senses = senses_dictionary[word_pair[1]]\n",
    "\n",
    "            word1_vectors = get_NASARI_vectors(word1_senses)\n",
    "            word2_vectors = get_NASARI_vectors(word2_senses)\n",
    "\n",
    "            similarity_value = max_cosine_similarity(word1_vectors, word2_vectors)[0]\n",
    "            similarity_dictionary[word_pair] = similarity_value\n",
    "\n",
    "        except KeyError:  # una delle due parole della coppia non c'è nel file \"SemEval17_IT_senses2synsets.txt\"\n",
    "            print(\"La coppia \", word_pair, \"non e' stata valutata\")\n",
    "    return similarity_dictionary\n",
    "\n",
    "# crea un dizionario che associa ad ogni coppia di parole del dizionario\n",
    "# delle annotazioni umane dei sensi, una coppia di babel synset che massimizza la similarità\n",
    "# del coseno tra l'insieme dei vettori associati alla prima parola\n",
    "# e l'insieme dei vettori associati alla seconda parola\n",
    "# CONSEGNA 2\n",
    "def get_word_pair_synset_pair_dictionary(human_synsets_dictionary, senses_dictionary):\n",
    "    synsets_dictionary = dict()\n",
    "    for word_pair in human_synsets_dictionary:\n",
    "        try:\n",
    "            word1_senses = senses_dictionary[word_pair[0]]\n",
    "            word2_senses = senses_dictionary[word_pair[1]]\n",
    "\n",
    "            word1_vectors = get_NASARI_vectors(word1_senses)\n",
    "            word2_vectors = get_NASARI_vectors(word2_senses)\n",
    "\n",
    "            # TO-DO\n",
    "            synset_pair = max_cosine_similarity(word1_vectors, word2_vectors)[1]\n",
    "            synsets_dictionary[word_pair] = synset_pair\n",
    "\n",
    "        except KeyError:  # una delle due parole della coppia non c'è nel file \"SemEval17_IT_senses2synsets.txt\"\n",
    "            print(\"La coppia \", word_pair, \"non e' stata valutata\")\n",
    "    return synsets_dictionary\n",
    "\n",
    "# cerca un vettore NASARI per ogni babel synset in input\n",
    "# associati ad una parola. Resituisce un dizionario\n",
    "# che avrà come chiavi i babel synset e come valori\n",
    "# i vettori NASARI associati\n",
    "def get_NASARI_vectors(word_senses):\n",
    "    word_vectors = dict()\n",
    "\n",
    "    NASARI_file = open(\"mini_NASARI.tsv\", encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(NASARI_file, delimiter=\"\\t\")\n",
    "\n",
    "    for row in read_tsv:\n",
    "        babel_synset = row[0].split(\"__\")[0]\n",
    "        if babel_synset in word_senses:\n",
    "            vector = [float(val) for val in row[1:]]\n",
    "            word_vectors[babel_synset] = vector\n",
    "\n",
    "    NASARI_file.close()\n",
    "    return word_vectors\n",
    "\n",
    "# massimizza la cosine_similarity tra due liste di vettori distribuzionali\n",
    "def max_cosine_similarity(word_vector1, word_vectors2):\n",
    "    max_cos_similarity = 0\n",
    "    for babel_synset1 in word_vector1.keys():\n",
    "        for babel_synset2 in word_vectors2.keys():\n",
    "            cos_similarity = c_similarity(word_vector1[babel_synset1], word_vectors2[babel_synset2])\n",
    "            if cos_similarity > max_cos_similarity:\n",
    "                max_cos_similarity = cos_similarity\n",
    "                synset_pair = (babel_synset1, babel_synset2)\n",
    "    return max_cos_similarity, synset_pair\n",
    "\n",
    "# calcola la similarità del coseno tra due vettori numerici\n",
    "# restituisce quindi il rapporto tra il prodotto scalare dei due vettori e il\n",
    "# prodotto della loro norma\n",
    "def c_similarity(vect1, vect2):\n",
    "    numerator = numpy.dot(vect1, vect2)\n",
    "    denominator = numpy.linalg.norm(vect1) * numpy.linalg.norm(vect2)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf092fb1-31fd-4a65-8758-19695eacaec8",
   "metadata": {},
   "source": [
    "Mappa un cognome su uno dei 10 insiemi di coppie da annotare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a2c825-8321-4dab-a3fe-fd243b0d2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parisi         :\tcoppie nell'intervallo 451-500\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname):\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo \n",
    "\n",
    "input_name = \"Parisi\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1772928-d8c9-46d1-af30-f431e9361d92",
   "metadata": {},
   "source": [
    "Processo della prima annotazione manuale dell'utente\n",
    "\n",
    "Vengono esaminate le coppie di termini dal file \"it.test.data.txt\" in base al cognone (Parisi: coppie nell'intervallo 451-500) che vanno da INTERVAL_START a INTERVAL_END\n",
    "\n",
    "Il processo di annotazione è fatto in modo manuale dall'utente che assegna un punteggio di similarità da 0 a 4.\n",
    "\n",
    "0: Totally dissimilar and unrelated\n",
    "\n",
    "1: Dissimilar\n",
    "\n",
    "2: Slightly similar\n",
    "\n",
    "3: Similar\n",
    "\n",
    "4: Very similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19915f76-5409-4cb2-8bb2-100534169f5d",
   "metadata": {},
   "source": [
    "INTERVAL_START = 451\n",
    "INTERVAL_END = 500\n",
    "\n",
    "with open('annotations1.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    with open(\"it.test.data.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i >= INTERVAL_START - 1:\n",
    "                line = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                print(\"==============================\")\n",
    "                word1, word2 = line[0], line[1]\n",
    "                print(word1, \" \", word2)\n",
    "                similarity_value = float(input(\"Inserire similaritÃ (0-4): \"))\n",
    "                similarity_value = format(similarity_value, '.1f')\n",
    "                tsv_writer.writerow([word1, word2, similarity_value])\n",
    "                print(\"==============================\")\n",
    "            if i == INTERVAL_END - 1:\n",
    "                break\n",
    "    fp.close()\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba5896-484c-4cb7-9709-c14f74228bf3",
   "metadata": {},
   "source": [
    "Processo della seconda annotazione manuale dell'utente\n",
    "\n",
    "Anche in questo in caso vengono prese in considerazione le coppie di parole presenti nel file in base al cognome.\n",
    "\n",
    "Il processo di annotazione consiste nel predere la coppia di termini associarla alla coppia di bable synset ID e cosiderare, per entrambe i termini, i 3 sensi più simili presenti su WordNet, in questo ordine: Term1 Term2 BS1 BS2 Terms_in_BS1 Terms_in_BS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05dc2f0-5311-4f5c-80c0-55aa5730d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    tsv_writer.writerow(['recessione', 'PIL', 'bn:00066516n','bn:00037570n','condizione macroeconomica,fattori produttivi,crescita economica','pil, P.I.L., prodotto interno lordo'])\n",
    "    tsv_writer.writerow(['Cesare', 'Giulio Cesare', 'bn:00014550n','bn:00014550n','Gaio Giulio Cesare,Giulio CesareCaio Giulio Cesare,imperatore ','Gaio Giulio Cesare,Giulio CesareCaio Giulio Cesare ,imperatore '])\n",
    "    tsv_writer.writerow(['paziente', 'sessione', 'bn:00061017n','bn:00076588n',' ammalato,assistito ,cliente ','fase,periodo,sessione '])\n",
    "    tsv_writer.writerow(['comportamentismo', 'terapia', 'bn:00009659n','bn:00076843n','behaviourismo,psicologia,comportamento  ','guarigione ,malattie ,metodi  '])\n",
    "    tsv_writer.writerow(['imperatore', 'costituzione', 'bn:00014550n','bn:00059480n','Gaio Giulio Cesare,Giulio CesareCaio Giulio Cesare,imperatore ','organizzazione ,formazione ,organizzativo '])\n",
    "    tsv_writer.writerow(['matematico', 'spettacolo', 'bn:00053834n','bn:00067553n',' persona,ricerche,sperimentazioni  ','rivista ,Teatro di rivista ,spettacolo '])\n",
    "    tsv_writer.writerow(['entropia', 'informazione', 'bn:00031061n','bn:00046705n','casualità ,azzardo ,grandezza  ','Conoscenza,studio,dati '])\n",
    "    tsv_writer.writerow(['acqua di rose', 'olio di rosa', 'bn:00068288n','bn:00007010n','giulebbe ,bevanda,estratto di rosa ','petali ,rosa,estratto '])\n",
    "    tsv_writer.writerow(['agrume', 'pompelmo', 'bn:00019301n','bn:00019313n','frutti,piante,Rutaceae','Agrume ,buccia gialla ,amarognolo '])\n",
    "    tsv_writer.writerow(['Regina Vittoria', 'Inghilterra', 'bn:00065652n','bn:00013173n','regina ,Regno Unito, Imperatrice ','Gran Bretagna ,Regno Unito ,Irlanda del Nord '])\n",
    "    tsv_writer.writerow(['Giochi Olimpici', 'spirito', 'bn:00058910n','bn:00040370n','olimpiade ,Olimpiadi ,Manifestazione','fantasma ,spettro ,apparizione '])\n",
    "    tsv_writer.writerow(['vescovo', 'musulmano', 'bn:27267995n','bn:00055975n',' responsabile ,chiese ,cattolicesimo ','mussulmano ,islamico ,maomettano '])\n",
    "    tsv_writer.writerow(['uomo', 'sospetto', 'bn:00044576n','bn:00025884n','essere umano , Homo,umano ','convenuto ,accusato ,imputato '])\n",
    "    tsv_writer.writerow(['meteorite', 'Terra', 'bn:00054602n','bn:00029424n','meteorite ,Oggetto ,spazio  ','mondo ,globo ,terrestre '])\n",
    "    tsv_writer.writerow(['simbolo', 'segno', 'bn:00075652n','bn:00075652n','segno ,significato convenzionale ,elemento ','segno ,significato convenzionale ,elemento '])\n",
    "    tsv_writer.writerow(['antropologia', 'New York', 'bn:00004584n','bn:00041611n','Scienza ,uomo ,entità biologica ','Nuova York ,stato di New York , città'])\n",
    "    tsv_writer.writerow(['tramonto', 'tavolo', 'bn:05692316n','bn:00075813n','crepuscolo ,notte ,illuminazione ','tavola ,mobilio ,piano '])\n",
    "    tsv_writer.writerow(['cittadina', 'citta', 'bn:00070724n','bn:00077773n','Insediamento umano ,villaggio ,borgo ','città ,cittadina ,paese '])\n",
    "    tsv_writer.writerow(['giacca', 'acqua minerale', 'bn:00047823n','bn:00055131n','cappotto ,casacca ,giacchetta ','minerale ,bottiglia ,acqua sorgiva '])\n",
    "    tsv_writer.writerow(['natura', 'flora', 'bn:00057017n','bn:00035324n','fenomeni ,forze ,cose ','pianta ,Plantae ,vegetale '])\n",
    "    tsv_writer.writerow(['subroutine', 'compilatore','bn:00036826n','bn:00021344n','sottoprogramma ,procedura ,procedimento ','Programma ,linguaggio ,istruzioni '])\n",
    "    tsv_writer.writerow(['Hamadan', 'Roma', 'bn:03266645n','bn:00015556n','città ,Iran ,Ecbatana ','Città Eterna ,Comune di Roma ,impero romano '])\n",
    "    tsv_writer.writerow(['ombrello', 'stufa', 'bn:00078920n','bn:00074479n','parapioggia ,paracqua ,ombrellone ','fornello ,fuoco , Apparecchio'])\n",
    "    tsv_writer.writerow(['onore', 'stima', 'bn:00027103n','bn:00031973n','dignità ,decoro ,onore ','valutazione ,estimazione ,perizia '])\n",
    "    tsv_writer.writerow(['insegna', 'dignita', 'bn:00034960n','bn:00027103n','bandiera ,segnalazioni , identificazione ','decoro, onore ,reputazione   '])\n",
    "    tsv_writer.writerow(['KFC', 'McDonalds', 'bn:01826071n','bn:01826071n',' catena ,ristoranti ,fast food  ',' catena ,ristoranti ,fast food  '])\n",
    "    tsv_writer.writerow(['joystick', 'radar', 'bn:00022301n','bn:00054808n','cloche ,barra ,comando ','radiolocalizzatore ,posizione  ,rilevamento  '])\n",
    "    tsv_writer.writerow(['basmati', 'riso jasmine', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['medaglia', 'scarpe da ginnastica', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['legge', 'piscina', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['sorgente', 'scatola', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['teatro', 'batteria', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['flora', 'web browser', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['camicia', 'cardigan', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['poema', 'ritmo', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['profeta', 'prete', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['Oscar', 'stadio', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['backgammon', 'Go', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['farfalla', 'rosa', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['recinto', 'salto', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['nichilismo', 'film', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['asteroide', 'stella', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['sommossa', 'disegno', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['intimo', 'corpo', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['Boeing', 'aereo', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['cameo', 'interpretazione', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['semestre', 'quadrimestre', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['arancia', 'agrume', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['ghiacciaio', 'riscaldamento globale', '','',' , , ',' , , '])\n",
    "    tsv_writer.writerow(['galleria', 'percorso', '','',' , , ',' , , '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d458e8-e56f-46d9-829d-4b0dc8d1cdd1",
   "metadata": {},
   "source": [
    "CONSEGNA 1: Consiste nell’annotare con punteggio di semantic similarity 50 coppie di termini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0345523-9f45-4c37-bce8-b12d33070db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b6a7e3dfd9f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Spearman Correlation: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNASARI_similarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-b6a7e3dfd9f8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mhuman_similarities_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_similarities_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m#print(human_similarities_dictionary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msenses_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_senses_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_word_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(senses_dictionary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-36b0be25956a>\u001b[0m in \u001b[0;36mget_human_similarities_dictionary\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mread_tsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# verifico che non sia una riga vuota (può capitare)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mhuman_similarities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mannotations_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    human_similarities_dictionary = get_human_similarities_dictionary()\n",
    "    senses_dictionary = get_senses_dictionary(get_word_list(human_similarities_dictionary))\n",
    "    NASARI_similarities_dictionary = get_NASARI_similarities_dictionary(human_similarities_dictionary, senses_dictionary)\n",
    "    \n",
    "    human_similarities = []\n",
    "    NASARI_similarities = []\n",
    "    for word_pair in human_similarities_dictionary.keys():\n",
    "        if word_pair in NASARI_similarities_dictionary.keys():\n",
    "            human_similarities.append(float(human_similarities_dictionary[word_pair]))\n",
    "            NASARI_similarities.append(NASARI_similarities_dictionary[word_pair])\n",
    "    print()\n",
    "    print(\"VALUTAZIONI DI SIMILARITA' UMANE: \")\n",
    "    print(human_similarities_dictionary)\n",
    "    print()\n",
    "    print(\"VALUTAZIONI DI SIMILARITA' DEL SISTEMA: \")\n",
    "    print(NASARI_similarities_dictionary)\n",
    "    print()\n",
    "    print(\"Pearson Correlation: \",np.corrcoef(human_similarities, NASARI_similarities))\n",
    "    print(\"Spearman Correlation: \",sp.stats.spearmanr(human_similarities, NASARI_similarities))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acaba0-3ca5-4b9a-b405-f8f10391425f",
   "metadata": {},
   "source": [
    "CONSEGNA 2: Consiste nell’individuare i sensi selezionati nel giudizio di similarità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb78449-3aae-434a-ae25-8dcc3673a0b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-bc67d7e386e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuratezza sulle coppie: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchecked\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mevaluated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-98-bc67d7e386e6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mhuman_similarities_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_similarities_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msenses_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_senses_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_word_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhuman_synsets_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_synsets_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mword_pair_synset_pair_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_pair_synset_pair_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_synsets_dictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msenses_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-a7caa4f5f5fe>\u001b[0m in \u001b[0;36mget_human_synsets_dictionary\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mread_tsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# verifico che non sia una riga vuota (può capitare)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mhuman_synsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mannotations_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    human_similarities_dictionary = get_human_similarities_dictionary()\n",
    "    senses_dictionary = get_senses_dictionary(get_word_list(human_similarities_dictionary))\n",
    "    human_synsets_dictionary = get_human_synsets_dictionary()\n",
    "    word_pair_synset_pair_dictionary = get_word_pair_synset_pair_dictionary(human_synsets_dictionary, senses_dictionary)\n",
    "    \n",
    "    print()\n",
    "    print(\"ASSEGNAMENTI SYNSETS UMANI: \")\n",
    "    print(human_synsets_dictionary)\n",
    "    print()\n",
    "    print(\"'ASSEGNAMENTI SYNSETS DEL SISTEMA: \")\n",
    "    print(word_pair_synset_pair_dictionary)\n",
    "    \n",
    "    print()\n",
    "    #calcolo accuratezza sui singoli elementi\n",
    "    checked = 0\n",
    "    for word_pair in human_synsets_dictionary.keys():\n",
    "        synset_pair = word_pair_synset_pair_dictionary[word_pair]\n",
    "        human_synsets_pair = human_synsets_dictionary[word_pair]\n",
    "        if synset_pair[0] == human_synsets_pair[0]:\n",
    "            checked += 1\n",
    "        if synset_pair[1] == human_synsets_pair[1]:\n",
    "            checked += 1\n",
    "    evaluated = len(human_synsets_dictionary.keys()) * 2\n",
    "    print(\"Accuratezza sui singoli elmenti: \", checked / evaluated)\n",
    "    \n",
    "    #calcolo accuratezza sulle coppie\n",
    "    checked = 0\n",
    "    for word_pair in human_synsets_dictionary.keys():\n",
    "        synset_pair = word_pair_synset_pair_dictionary[word_pair]\n",
    "        human_synsets_pair = human_synsets_dictionary[word_pair]\n",
    "        if (synset_pair[0] == human_synsets_pair[0]) and (synset_pair[1] == human_synsets_pair[1]):\n",
    "            checked += 1\n",
    "    evaluated = len(human_synsets_dictionary.keys())\n",
    "    print(\"Accuratezza sulle coppie: \", checked / evaluated)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
