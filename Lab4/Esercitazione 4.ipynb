{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fe34c2c-0a02-4991-a579-f6fadf5955ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0797ee-e03f-462c-9f8b-eee35889babe",
   "metadata": {},
   "source": [
    "FUNZIONI UTILI PER L'ANALISI DEL TESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31130c55-85df-445c-81cb-af7cb83802cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un dizionario, in cui ad ogni parola (chiave) corrisponde una lista\n",
    "# di babel synset presi del file \"SemEval17_IT_senses2sysensts.txt\"\n",
    "def get_senses_dictionary(word_list):\n",
    "    senses_for_words = dict()\n",
    "    file = open(\"SemEval17_IT_senses2synsets.txt\", \"r\", encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, len(lines)):\n",
    "        line = lines[i]\n",
    "        line = line.replace(\"\\n\", \"\").replace(\"#\", \"\")\n",
    "        if line in word_list:\n",
    "            while True:\n",
    "                i += 1\n",
    "                babel_synset = lines[i].replace(\"\\n\", \"\")\n",
    "                if \"#\" in babel_synset:  # vuol dire che non sto considerando più un babel synset\n",
    "                    break\n",
    "                if line not in senses_for_words:\n",
    "                    senses_for_words[line] = [babel_synset]\n",
    "                else:\n",
    "                    senses_for_words[line].append(babel_synset)\n",
    "    return senses_for_words\n",
    "\n",
    "# crea un dizionario delle annotazioni, in cui ad ogni coppia di parole\n",
    "# prese dal file delle annotazioni \"annotations1.tsv\" associa il valore\n",
    "# di similarità annotato da un essere umano\n",
    "def get_human_similarities_dictionary():\n",
    "    human_similarities = dict()\n",
    "    annotations_file = open(\"annotations1.tsv\")\n",
    "    read_tsv = csv.reader(annotations_file, delimiter=\"\\t\")\n",
    "    for row in read_tsv:\n",
    "        if row[0]: #verifico che non sia una riga vuota (può capitare)\n",
    "            human_similarities[(row[0],row[1])] = row[2]\n",
    "    annotations_file.close()\n",
    "    return human_similarities\n",
    "\n",
    "# crea un dizionario delle annotazioni, in cui ad ogni coppia di parola\n",
    "# prese dal file delle annotazioni \"annotations2.tsv\" associa una coppia\n",
    "# di BABEL synset annotati da un essere umano sulla base delle annotazioni\n",
    "# di similarità del file \"annotations1.tsv\"\n",
    "def get_human_synsets_dictionary():\n",
    "    human_synsets = dict()\n",
    "    annotations_file = open(\"annotations2.tsv\")\n",
    "    read_tsv = csv.reader(annotations_file, delimiter=\"\\t\")\n",
    "    for row in read_tsv:\n",
    "        if row[0]: #verifico che non sia una riga vuota (può capitare)\n",
    "            human_synsets[((row[0],row[1]))] = (row[2],row[3])\n",
    "    annotations_file.close()\n",
    "    return human_synsets\n",
    "\n",
    "# restituisce tutte le parole presenti nella coppie valutate nel dizionario\n",
    "# delle annotazioni umane che viene dato in input\n",
    "def get_word_list(human_similarities_dictionary):\n",
    "    word_list = []\n",
    "    for pair in human_similarities_dictionary.keys():\n",
    "        word_list.append(pair[0])\n",
    "        word_list.append(pair[1])\n",
    "    return word_list\n",
    "\n",
    "# crea una dizionario che associa ad ogni coppia di parole del dizionario\n",
    "# delle annotazioni umane delle similarità, un valore di similarità dato dalla massimizzazione della similarità\n",
    "# del coseno tra l'insieme dei vettori nasari associati ad una parola\n",
    "# e l'insieme dei vettori nasari associati all'altra parola\n",
    "# CONSEGNA 1\n",
    "def get_NASARI_similarities_dictionary(human_similarities_dictionary, senses_dictionary):\n",
    "    similarity_dictionary = dict()\n",
    "    for word_pair in human_similarities_dictionary:\n",
    "\n",
    "        try:\n",
    "            word1_senses = senses_dictionary[word_pair[0]]\n",
    "            word2_senses = senses_dictionary[word_pair[1]]\n",
    "\n",
    "            word1_vectors = get_NASARI_vectors(word1_senses)\n",
    "            word2_vectors = get_NASARI_vectors(word2_senses)\n",
    "\n",
    "            similarity_value = max_cosine_similarity(word1_vectors, word2_vectors)[0]\n",
    "            similarity_dictionary[word_pair] = similarity_value\n",
    "\n",
    "        except KeyError:  # una delle due parole della coppia non c'è nel file \"SemEval17_IT_senses2synsets.txt\"\n",
    "            print(\"La coppia \", word_pair, \"non e' stata valutata\")\n",
    "    return similarity_dictionary\n",
    "\n",
    "# crea un dizionario che associa ad ogni coppia di parole del dizionario\n",
    "# delle annotazioni umane dei sensi, una coppia di babel synset che massimizza la similarità\n",
    "# del coseno tra l'insieme dei vettori associati alla prima parola\n",
    "# e l'insieme dei vettori associati alla seconda parola\n",
    "# CONSEGNA 2\n",
    "def get_word_pair_synset_pair_dictionary(human_synsets_dictionary, senses_dictionary):\n",
    "    synsets_dictionary = dict()\n",
    "    for word_pair in human_synsets_dictionary:\n",
    "        try:\n",
    "            word1_senses = senses_dictionary[word_pair[0]]\n",
    "            word2_senses = senses_dictionary[word_pair[1]]\n",
    "\n",
    "            word1_vectors = get_NASARI_vectors(word1_senses)\n",
    "            word2_vectors = get_NASARI_vectors(word2_senses)\n",
    "\n",
    "            # TO-DO\n",
    "            synset_pair = max_cosine_similarity(word1_vectors, word2_vectors)[1]\n",
    "            synsets_dictionary[word_pair] = synset_pair\n",
    "\n",
    "        except KeyError:  # una delle due parole della coppia non c'è nel file \"SemEval17_IT_senses2synsets.txt\"\n",
    "            print(\"La coppia \", word_pair, \"non e' stata valutata\")\n",
    "    return synsets_dictionary\n",
    "\n",
    "# cerca un vettore NASARI per ogni babel synset in input\n",
    "# associati ad una parola. Resituisce un dizionario\n",
    "# che avrà come chiavi i babel synset e come valori\n",
    "# i vettori NASARI associati\n",
    "def get_NASARI_vectors(word_senses):\n",
    "    word_vectors = dict()\n",
    "\n",
    "    NASARI_file = open(\"mini_NASARI.tsv\", encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(NASARI_file, delimiter=\"\\t\")\n",
    "\n",
    "    for row in read_tsv:\n",
    "        babel_synset = row[0].split(\"__\")[0]\n",
    "        if babel_synset in word_senses:\n",
    "            vector = [float(val) for val in row[1:]]\n",
    "            word_vectors[babel_synset] = vector\n",
    "\n",
    "    NASARI_file.close()\n",
    "    return word_vectors\n",
    "\n",
    "# massimizza la cosine_similarity tra due liste di vettori distribuzionali\n",
    "def max_cosine_similarity(word_vector1, word_vectors2):\n",
    "    max_cos_similarity = 0\n",
    "    for babel_synset1 in word_vector1.keys():\n",
    "        for babel_synset2 in word_vectors2.keys():\n",
    "            cos_similarity = c_similarity(word_vector1[babel_synset1], word_vectors2[babel_synset2])\n",
    "            if cos_similarity > max_cos_similarity:\n",
    "                max_cos_similarity = cos_similarity\n",
    "                synset_pair = (babel_synset1, babel_synset2)\n",
    "    return max_cos_similarity, synset_pair\n",
    "\n",
    "# calcola la similarità del coseno tra due vettori numerici\n",
    "# restituisce quindi il rapporto tra il prodotto scalare dei due vettori e il\n",
    "# prodotto della loro norma\n",
    "def c_similarity(vect1, vect2):\n",
    "    numerator = numpy.dot(vect1, vect2)\n",
    "    denominator = numpy.linalg.norm(vect1) * numpy.linalg.norm(vect2)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf092fb1-31fd-4a65-8758-19695eacaec8",
   "metadata": {},
   "source": [
    "Mappa un cognome su uno dei 10 insiemi di coppie da annotare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31a2c825-8321-4dab-a3fe-fd243b0d2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parisi         :\tcoppie nell'intervallo 451-500\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname):\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo \n",
    "\n",
    "input_name = \"Parisi\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1772928-d8c9-46d1-af30-f431e9361d92",
   "metadata": {},
   "source": [
    "Processo della prima annotazione manuale dell'utente\n",
    "\n",
    "Vengono esaminate le coppie di termini dal file \"it.test.data.txt\" in base al cognone (Parisi: coppie nell'intervallo 451-500) che vanno da INTERVAL_START a INTERVAL_END\n",
    "\n",
    "Il processo di annotazione è fatto in modo manuale dall'utente che assegna un punteggio di similarità da 0 a 4.\n",
    "\n",
    "0: Totally dissimilar and unrelated\n",
    "\n",
    "1: Dissimilar\n",
    "\n",
    "2: Slightly similar\n",
    "\n",
    "3: Similar\n",
    "\n",
    "4: Very similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f719bfaa-91d1-435b-9f28-d312dca33e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "recessione   PIL\n",
      "==============================\n",
      "==============================\n",
      "Cesare   Giulio Cesare\n",
      "==============================\n",
      "==============================\n",
      "paziente   sessione\n",
      "==============================\n",
      "==============================\n",
      "comportamentismo   terapia\n",
      "==============================\n",
      "==============================\n",
      "imperatore   costituzione\n",
      "==============================\n",
      "==============================\n",
      "matematico   spettacolo\n",
      "==============================\n",
      "==============================\n",
      "entropia   informazione\n",
      "==============================\n",
      "==============================\n",
      "acqua di rose   olio di rosa\n",
      "==============================\n",
      "==============================\n",
      "agrume   pompelmo\n",
      "==============================\n",
      "==============================\n",
      "Regina Vittoria   Inghilterra\n",
      "==============================\n",
      "==============================\n",
      "Giochi Olimpici   spirito\n",
      "==============================\n",
      "==============================\n",
      "vescovo   musulmano\n",
      "==============================\n",
      "==============================\n",
      "uomo   sospetto\n",
      "==============================\n",
      "==============================\n",
      "meteorite   Terra\n",
      "==============================\n",
      "==============================\n",
      "simbolo   segno\n",
      "==============================\n",
      "==============================\n",
      "antropologia   New York\n",
      "==============================\n",
      "==============================\n",
      "tramonto   tavolo\n",
      "==============================\n",
      "==============================\n",
      "cittadina   città\n",
      "==============================\n",
      "==============================\n",
      "giacca   acqua minerale\n",
      "==============================\n",
      "==============================\n",
      "natura   flora\n",
      "==============================\n",
      "==============================\n",
      "subroutine   compilatore\n",
      "==============================\n",
      "==============================\n",
      "Hamadan   Roma\n",
      "==============================\n",
      "==============================\n",
      "ombrello   stufa\n",
      "==============================\n",
      "==============================\n",
      "onore   stima\n",
      "==============================\n",
      "==============================\n",
      "insegna   dignità\n",
      "==============================\n",
      "==============================\n",
      "KFC   McDonald's\n",
      "==============================\n",
      "==============================\n",
      "joystick   radar\n",
      "==============================\n",
      "==============================\n",
      "basmati   riso jasmine\n",
      "==============================\n",
      "==============================\n",
      "medaglia   scarpe da ginnastica\n",
      "==============================\n",
      "==============================\n",
      "legge   piscina\n",
      "==============================\n",
      "==============================\n",
      "sorgente   scatola\n",
      "==============================\n",
      "==============================\n",
      "teatro   batteria\n",
      "==============================\n",
      "==============================\n",
      "flora   web browser\n",
      "==============================\n",
      "==============================\n",
      "camicia   cardigan\n",
      "==============================\n",
      "==============================\n",
      "poema   ritmo\n",
      "==============================\n",
      "==============================\n",
      "profeta   prete\n",
      "==============================\n",
      "==============================\n",
      "Oscar   stadio\n",
      "==============================\n",
      "==============================\n",
      "backgammon   Go\n",
      "==============================\n",
      "==============================\n",
      "farfalla   rosa\n",
      "==============================\n",
      "==============================\n",
      "recinto   salto\n",
      "==============================\n",
      "==============================\n",
      "nichilismo   film\n",
      "==============================\n",
      "==============================\n",
      "asteroide   stella\n",
      "==============================\n",
      "==============================\n",
      "sommossa   disegno\n",
      "==============================\n",
      "==============================\n",
      "intimo   corpo\n",
      "==============================\n",
      "==============================\n",
      "Boeing   aereo\n",
      "==============================\n",
      "==============================\n",
      "cameo   interpretazione\n",
      "==============================\n",
      "==============================\n",
      "semestre   quadrimestre\n",
      "==============================\n",
      "==============================\n",
      "arancia   agrume\n",
      "==============================\n",
      "==============================\n",
      "ghiacciaio   riscaldamento globale\n",
      "==============================\n",
      "==============================\n",
      "galleria   percorso\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "INTERVAL_START = 451\n",
    "INTERVAL_END = 500\n",
    "\n",
    "with open('annotations1.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    with open(\"it.test.data.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i >= INTERVAL_START - 1:\n",
    "                line = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                print(\"==============================\")\n",
    "                word1, word2 = line[0], line[1]\n",
    "                print(word1, \" \", word2)\n",
    "                similarity_value = float(input(\"Inserire similaritÃ (0-4): \"))\n",
    "                similarity_value = format(similarity_value, '.1f')\n",
    "                tsv_writer.writerow([word1, word2, similarity_value])\n",
    "                print(\"==============================\")\n",
    "            if i == INTERVAL_END - 1:\n",
    "                break\n",
    "    fp.close()\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba5896-484c-4cb7-9709-c14f74228bf3",
   "metadata": {},
   "source": [
    "Processo della seconda annotazione manuale dell'utente\n",
    "\n",
    "Anche in questo in caso vengono prese in considerazione le coppie di parole presenti nel file in base al cognome.\n",
    "\n",
    "Il processo di annotazione consiste nel predere la coppia di termini associarla alla coppia di bable synset ID e cosiderare, per entrambe i termini, i 3 sensi più simili presenti su WordNet, in questo ordine: Term1 Term2 BS1 BS2 Terms_in_BS1 Terms_in_BS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d05dc2f0-5311-4f5c-80c0-55aa5730d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    tsv_writer.writerow(['recessione', 'PIL', 'bn:00066516n','bn:00037570n','condizione macroeconomica,fattori produttivi,crescita economica','pil, P.I.L.,prodotto interno lordo'])\n",
    "    tsv_writer.writerow(['Cesare', 'Giulio Cesare', 'bn:00014550n','bn:00014550n','Gaio Giulio Cesare,Caio Giulio Cesare,imperatore ','Gaio Giulio Cesare,Caio Giulio Cesare ,imperatore'])\n",
    "    tsv_writer.writerow(['paziente', 'sessione', 'bn:00061017n','bn:00076588n',' ammalato,assistito,cliente ','fase,periodo,sessione'])\n",
    "    tsv_writer.writerow(['comportamentismo', 'terapia', 'bn:00009659n','bn:00076843n','behaviourismo,psicologia,comportamento  ','guarigione,malattie,metodi'])\n",
    "    tsv_writer.writerow(['imperatore', 'costituzione', 'bn:00014550n','bn:00059480n','Gaio Giulio Cesare,Caio Giulio Cesare,imperatore','organizzazione,formazione,organizzativo'])\n",
    "    tsv_writer.writerow(['matematico', 'spettacolo', 'bn:00053834n','bn:00067553n','persona,ricerche,sperimentazioni','rivista ,Teatro di rivista,spettacolo '])\n",
    "    tsv_writer.writerow(['entropia', 'informazione', 'bn:00031061n','bn:00046705n','casualita,azzardo,grandezza','Conoscenza,studio,dati '])\n",
    "    tsv_writer.writerow(['acqua di rose', 'olio di rosa', 'bn:00068288n','bn:00007010n','giulebbe,bevanda,estratto di rosa ','petali,rosa,estratto '])\n",
    "    tsv_writer.writerow(['agrume', 'pompelmo', 'bn:00019301n','bn:00019313n','frutti,piante,Rutaceae','Agrume,buccia gialla,amarognolo '])\n",
    "    tsv_writer.writerow(['Regina Vittoria', 'Inghilterra', 'bn:00065652n','bn:00013173n','regina,Regno Unito,Imperatrice ','Gran Bretagna,Regno Unito,Irlanda del Nord '])\n",
    "    tsv_writer.writerow(['Giochi Olimpici', 'spirito', 'bn:00058910n','bn:00040370n','olimpiade,Olimpiadi ,Manifestazione','fantasma,spettro,apparizione '])\n",
    "    tsv_writer.writerow(['vescovo', 'musulmano', 'bn:27267995n','bn:00055975n',' responsabile,chiese,cattolicesimo ','mussulmano,islamico,maomettano'])\n",
    "    tsv_writer.writerow(['uomo', 'sospetto', 'bn:00044576n','bn:00025884n','essere umano,Homo,umano ','convenuto,accusato ,imputato '])\n",
    "    tsv_writer.writerow(['meteorite', 'Terra', 'bn:00054602n','bn:00029424n','meteorite,Oggetto,spazio  ','mondo ,globo,terrestre '])\n",
    "    tsv_writer.writerow(['simbolo', 'segno', 'bn:00075652n','bn:00075652n','segno,significato convenzionale,elemento ','segno,significato convenzionale,elemento'])\n",
    "    tsv_writer.writerow(['antropologia', 'New York', 'bn:00004584n','bn:00041611n','Scienza,uomo,entita biologica ','Nuova York ,stato di New York,citta'])\n",
    "    tsv_writer.writerow(['tramonto', 'tavolo', 'bn:05692316n','bn:00075813n','crepuscolo,notte,illuminazione','tavola,mobilio,piano'])\n",
    "    tsv_writer.writerow(['cittadina', 'citta', 'bn:00070724n','bn:00077773n','Insediamento umano,villaggio,borgo ','citta,cittadina ,paese '])\n",
    "    tsv_writer.writerow(['giacca', 'acqua minerale', 'bn:00047823n','bn:00055131n','cappotto,casacca,giacchetta ','minerale,bottiglia,acqua sorgiva '])\n",
    "    tsv_writer.writerow(['natura', 'flora', 'bn:00057017n','bn:00035324n','fenomeni,forze,cose','pianta,Plantae,vegetale '])\n",
    "    tsv_writer.writerow(['subroutine', 'compilatore','bn:00036826n','bn:00021344n','sottoprogramma,procedura,procedimento','Programma,linguaggio,istruzioni'])\n",
    "    tsv_writer.writerow(['Hamadan', 'Roma', 'bn:03266645n','bn:00015556n','citta,Iran,Ecbatana','Citta Eterna,Comune di Roma,impero romano '])\n",
    "    tsv_writer.writerow(['ombrello', 'stufa', 'bn:00078920n','bn:00074479n','parapioggia,paracqua,ombrellone','fornello,fuoco,apparecchio'])\n",
    "    tsv_writer.writerow(['onore', 'stima', 'bn:00027103n','bn:00031973n','dignita,decoro,onore','valutazione,estimazione,perizia'])\n",
    "    tsv_writer.writerow(['insegna', 'dignita', 'bn:00034960n','bn:00027103n','bandiera,segnalazioni,identificazione','decoro,onore,reputazione'])\n",
    "    tsv_writer.writerow(['KFC', 'McDonalds', 'bn:01826071n','bn:01826071n',' catena,ristoranti,fast food','catena,ristoranti,fast food'])\n",
    "    tsv_writer.writerow(['joystick', 'radar', 'bn:00022301n','bn:00054808n','cloche,barra,comando','radiolocalizzatore,posizione,rilevamento'])\n",
    "    tsv_writer.writerow(['basmati', 'riso jasmine', 'bn:00567353n','bn:02666084n','riso,grano,fragranza ','riso,chicco lungo,fiori'])\n",
    "    tsv_writer.writerow(['medaglia', 'scarpe da ginnastica', 'bn:23946490n','bn:00042319n','riconoscimento,Attestazione,distinzione','sneaker,takkies,attivita sportive'])\n",
    "    tsv_writer.writerow(['legge', 'piscina', 'bn:00048655n','bn:00056911n','diritto,giurisprudenza,legislazione','vasca,piscina coperta,nuotare'])\n",
    "    tsv_writer.writerow(['sorgente', 'scatola', 'bn:00036077n','bn:00012524n','fonte,polla,scaturigine','cassetta,astuccio,cassa'])\n",
    "    tsv_writer.writerow(['teatro', 'batteria', 'bn:00045002n','bn:00028891n','Edificio,opere,spettacolo ','tamburo,Membranofoni,membranofono '])\n",
    "    tsv_writer.writerow(['flora', 'web browser', 'bn:00035324n','bn:00013447n','pianta,Plantae,vegetale','browser,navigatore,informatica'])\n",
    "    tsv_writer.writerow(['camicia', 'cardigan', 'bn:00071142n','bn:00015958n','Indumento,cotone,maniche','golf,maglione,bottoni'])\n",
    "    tsv_writer.writerow(['poema', 'ritmo', 'bn:15205441n','bn:00009396n','poesia,versi,narrativo','tempo,pause,intervalli'])\n",
    "    tsv_writer.writerow(['profeta', 'prete', 'bn:00064759n','bn:00024459n','aedo,divinatore,vate','pastore,curato,Parroco'])\n",
    "    tsv_writer.writerow(['Oscar', 'stadio', 'bn:00000571n','bn:00005532n','Premio Oscar,Academy Award, cinema','arena,campo sportivo,palazzetto dello sport'])\n",
    "    tsv_writer.writerow(['backgammon', 'Go', 'bn:00007779n','bn:00040833n','tavola reale,sbaraglino,tric-trac','gioco da tavolo,giocatori ,Cina'])\n",
    "    tsv_writer.writerow(['farfalla', 'rosa', 'bn:00014271n','bn:00068283n','Insetto ,grandi ali ,Lepidotteri','rosaio,Fiore,pianta'])\n",
    "    tsv_writer.writerow(['recinto', 'salto', 'bn:00034048n','bn:00048558n','recinzione,barriera,cinta','movimento,salto,abilita'])\n",
    "    tsv_writer.writerow(['nichilismo', 'film', 'bn:00057710n','bn:00034471n','nihilismo,Dottrina filosofica ,societa ','spettacolo ,pellicola,opera cinematografica'])\n",
    "    tsv_writer.writerow(['asteroide', 'stella', 'bn:00006608n','bn:00073964n','planetoide,corpo celeste,pianeta terrestre','stella,sole,Corpo celeste'])\n",
    "    tsv_writer.writerow(['sommossa', 'disegno', 'bn:00047003n','bn:00028639n','ribellione,rivolta,insurrezione','figura,illustrazione ,riproduzione'])\n",
    "    tsv_writer.writerow(['intimo', 'corpo', 'bn:00079003n','bn:00011744n','biancheria intima,lingerie,indumento','organismo,tessuti,tessuti'])\n",
    "    tsv_writer.writerow(['Boeing', 'aereo', 'bn:01156257n','bn:00002275n','aereo militare,mezzo militare,spazioplano','aeromobile,velivolo,aeroplano'])\n",
    "    tsv_writer.writerow(['cameo', 'interpretazione', 'bn:02119437n','bn:00061560n','apparizione,personaggio famoso,spettacolo','performance,esecuzione,esibizione'])\n",
    "    tsv_writer.writerow(['semestre', 'quadrimestre', 'bn:00000561n','bn:15658281n','suddivisione anno, sei mesi ,scolastico','suddivisione anno, quattro mesi,scolastico'])\n",
    "    tsv_writer.writerow(['arancia', 'agrume', 'bn:17389700n','bn:00019301n','arancio ,albero,Rutaceae','Citrus,frutto,pianta'])\n",
    "    tsv_writer.writerow(['ghiacciaio', 'riscaldamento globale', 'bn:00040579n','bn:00040681n','ghiaccio,regioni montane,neve','clima,atmosfera,mutamento'])\n",
    "    tsv_writer.writerow(['galleria', 'percorso', 'bn:00078606n','bn:00067975n','tunnel,Passaggio,cunicolo','strada,via,Striscia di terreno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d458e8-e56f-46d9-829d-4b0dc8d1cdd1",
   "metadata": {},
   "source": [
    "CONSEGNA 1: Consiste nell’annotare con punteggio di semantic similarity 50 coppie di termini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0345523-9f45-4c37-bce8-b12d33070db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-0f9c2f51671d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Spearman Correlation: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNASARI_similarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-0f9c2f51671d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mhuman_similarities_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_similarities_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msenses_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_senses_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_word_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mNASARI_similarities_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_NASARI_similarities_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_similarities_dictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msenses_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-cc59e01ab1ea>\u001b[0m in \u001b[0;36mget_human_similarities_dictionary\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mread_tsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#verifico che non sia una riga vuota (può capitare)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mhuman_similarities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mannotations_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    human_similarities_dictionary = get_human_similarities_dictionary()\n",
    "    senses_dictionary = get_senses_dictionary(get_word_list(human_similarities_dictionary))\n",
    "    NASARI_similarities_dictionary = get_NASARI_similarities_dictionary(human_similarities_dictionary, senses_dictionary)\n",
    "    \n",
    "    human_similarities = []\n",
    "    NASARI_similarities = []\n",
    "    for word_pair in human_similarities_dictionary.keys():\n",
    "        if word_pair in NASARI_similarities_dictionary.keys():\n",
    "            human_similarities.append(float(human_similarities_dictionary[word_pair]))\n",
    "            NASARI_similarities.append(NASARI_similarities_dictionary[word_pair])\n",
    "    print()\n",
    "    print(\"VALUTAZIONI DI SIMILARITA' UMANE: \")\n",
    "    print(human_similarities_dictionary)\n",
    "    print()\n",
    "    print(\"VALUTAZIONI DI SIMILARITA' DEL SISTEMA: \")\n",
    "    print(NASARI_similarities_dictionary)\n",
    "    print()\n",
    "    print(\"Pearson Correlation: \",np.corrcoef(human_similarities, NASARI_similarities))\n",
    "    print(\"Spearman Correlation: \",sp.stats.spearmanr(human_similarities, NASARI_similarities))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acaba0-3ca5-4b9a-b405-f8f10391425f",
   "metadata": {},
   "source": [
    "CONSEGNA 2: Consiste nell’individuare i sensi selezionati nel giudizio di similarità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb78449-3aae-434a-ae25-8dcc3673a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    human_similarities_dictionary = get_human_similarities_dictionary()\n",
    "    senses_dictionary = get_senses_dictionary(get_word_list(human_similarities_dictionary))\n",
    "    human_synsets_dictionary = get_human_synsets_dictionary()\n",
    "    word_pair_synset_pair_dictionary = get_word_pair_synset_pair_dictionary(human_synsets_dictionary, senses_dictionary)\n",
    "    \n",
    "    print()\n",
    "    print(\"ASSEGNAMENTI SYNSETS UMANI: \")\n",
    "    print(human_synsets_dictionary)\n",
    "    print()\n",
    "    print(\"'ASSEGNAMENTI SYNSETS DEL SISTEMA: \")\n",
    "    print(word_pair_synset_pair_dictionary)\n",
    "    \n",
    "    print()\n",
    "    #calcolo accuratezza sui singoli elementi\n",
    "    checked = 0\n",
    "    for word_pair in human_synsets_dictionary.keys():\n",
    "        synset_pair = word_pair_synset_pair_dictionary[word_pair]\n",
    "        human_synsets_pair = human_synsets_dictionary[word_pair]\n",
    "        if synset_pair[0] == human_synsets_pair[0]:\n",
    "            checked += 1\n",
    "        if synset_pair[1] == human_synsets_pair[1]:\n",
    "            checked += 1\n",
    "    evaluated = len(human_synsets_dictionary.keys()) * 2\n",
    "    print(\"Accuratezza sui singoli elmenti: \", checked / evaluated)\n",
    "    \n",
    "    #calcolo accuratezza sulle coppie\n",
    "    checked = 0\n",
    "    for word_pair in human_synsets_dictionary.keys():\n",
    "        synset_pair = word_pair_synset_pair_dictionary[word_pair]\n",
    "        human_synsets_pair = human_synsets_dictionary[word_pair]\n",
    "        if (synset_pair[0] == human_synsets_pair[0]) and (synset_pair[1] == human_synsets_pair[1]):\n",
    "            checked += 1\n",
    "    evaluated = len(human_synsets_dictionary.keys())\n",
    "    print(\"Accuratezza sulle coppie: \", checked / evaluated)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
