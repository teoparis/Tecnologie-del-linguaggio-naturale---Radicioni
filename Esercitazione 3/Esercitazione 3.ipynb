{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd2d621f-5b87-49dd-b996-cce75f2c6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09509e0-2614-4048-8b31-886bcfc046b0",
   "metadata": {},
   "source": [
    "FUNZIONI DI PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "967261cf-febf-42e5-8b1d-b645fa4b4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restituisce la bag of word per la frase o il paragrafo in oggetto\n",
    "#effettua il pre-processing che consiste nella tokenizzazione, lemmatizzazione,\n",
    "#rimozione della punteggiatura e delle stopwords di una sentence \n",
    "def bag_of_words(sentence):\n",
    "    return set(remove_stopwords(tokenize_sentence(remove_punctuation(sentence))))\n",
    "\n",
    "#rimuove le stowords da una lista di parole\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords_list = get_stopwords()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "#Rimuove la punteggiatura da una sentence\n",
    "#Restituisce la sentence senza punteggiature\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]','',sentence)\n",
    "\n",
    "#Tokenizza la frase in input e ne affettua anche la lemmatizzazione della sue parole\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos = wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos = wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos = wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos = wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "#Restituisce la l'insieme di stopwords dal file delle stopwords\n",
    "def get_stopwords():\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fabf3-55de-48c0-88ea-151e87c3f78a",
   "metadata": {},
   "source": [
    "FUNZIONI UTILI PER LA COSTRUZIONE DEI RIASSUNTO DEL TESTO DATO IN INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5de715f-b3e6-4220-b7d2-6774913e35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PARAGRAPH_LEN = 50\n",
    "\n",
    "\"\"\"Approccio TITLE\"\"\"\n",
    "#Metodo utilizzato nell'approccio TITLE\n",
    "#il topic viene preso dal primo paragrafo del documento che in generale è il titolo\n",
    "def get_title_topic(document):\n",
    "    title = document[0]\n",
    "    return get_Nasari_vectors_for_bag_of_words(bag_of_words(title))\n",
    "\n",
    "#Restituisce una lista di paragrafi del documento in input\n",
    "#il primo paragrafo rappresenta il titolo\n",
    "def parse_document(doc):\n",
    "    document = []\n",
    "    data = doc.read_text(encoding='utf-8')\n",
    "    lines = data.split('\\n')\n",
    "    \n",
    "    for index,line in enumerate(lines):\n",
    "        if line != \"\" and not \"#\" in line and (len(line) > MIN_PARAGRAPH_LEN or index == 3):\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            document.append(line)\n",
    "    return document\n",
    "\"\"\"Approccio TITLE\"\"\"\n",
    "\n",
    "\"\"\"Approccio CUE\"\"\"\n",
    "\n",
    "#Le stigma words ci permettono di capire che NON stanno per essere dette cose importanti\n",
    "def get_stigma_words():\n",
    "    return ['no','not','i','you','she','he','we','they','it','me','him','her','us','them','mine','ours',\n",
    "            'hers','theirs','ourselves','myself','himself','who','whose','which','what','this','that',\n",
    "            'these','those','whom','whose']\n",
    "\n",
    "#Le bonus words ci permettono di capire che stanno per essere dette cose importanti\n",
    "def get_bonus_words():\n",
    "    return  ['better', 'worse', 'less', 'more', 'further', 'farther', 'best', 'worst', 'least', 'most',\n",
    "             'furthest', 'farthest', 'more', 'important','seen', 'all', 'fact', 'final', 'analysis',\n",
    "             'whole', 'brief', 'altogether', 'obviously','overall', 'ultimately', 'ordinarily',\n",
    "             'definitely','usually', 'emphasize', 'result','henceforth', 'additionally', 'main', \n",
    "             'aim','purpose', 'outline', 'investigation']\n",
    "\n",
    "#Medoto utilizzato nell'approccio CUE\n",
    "#Restituisce il topic del paragrafo più importante del documento\n",
    "#il paragrafo più importante del documento è scelto in base alla prensenza di stigma word o bonus word\n",
    "#ad ogni paragrafo viene associato un punteggio che aumenta di 1 per ogni bonus word\n",
    "#e diminuisce di 1 per ogni stigma word al suo interno\n",
    "#viene stilato un ranking e come topic viene scelto il paragrafo con il punteggio più alto\n",
    "def get_topic(document):\n",
    "    paragraph_score = []\n",
    "    for paragraph in document:\n",
    "        paragraph_score.append((paragraph, get_CUE_score(paragraph)))\n",
    "    more_important_paragraph =  sorted(paragraph_score, key=lambda x: x[1], reverse = True)[0] #prendo il primo in classifica\n",
    "    print(\"\\nPARAGRAFO PIU' IMPORTANTE: \\n\", more_important_paragraph)\n",
    "    print(bag_of_words(more_important_paragraph[0]))\n",
    "    return get_Nasari_vectors_for_bag_of_words(bag_of_words(more_important_paragraph[0]))\n",
    "\n",
    "#Restituisce uno score per il paragrafo in input\n",
    "#direttamente proporzionale alle bonus word e inversamente proporzionale alle stigma word\n",
    "#lo score è un numero intero positivo o negativo\n",
    "def get_CUE_score(paragraph):\n",
    "    word_list = tokenize_sentence(remove_punctuation(paragraph))\n",
    "    score = 0\n",
    "    for word in word_list:\n",
    "        if word in get_bonus_words(): score += 1\n",
    "        elif word in get_stigma_words(): score -= 1\n",
    "    return score\n",
    "\"\"\"Approccio CUE\"\"\"\n",
    "\n",
    "#riceve in input una riga del file NASARI small e restituisce\n",
    "#un vettore NASARI formattato, per esempio, nel seguente modo:\n",
    "#[('million', '209.35'), ('number', '146.31'), ('mathematics', '61.3'), \n",
    "#('long scale', '53.31'), ('real number', '50.43'), ('numeral', '50.35'), \n",
    "#('short scale', '50.12'), ('digit', '42.17'), ('bally', '41.77'), ('millionaire', '41.31'), \n",
    "#('penguin', '41.11'), ('markov', '40.61'), ('complex number', '38.37'), ('infinity', '36.79')]\n",
    "def vector_format(nasari_line):\n",
    "    line_splitted = nasari_line.replace(\"\\n\", \"\").split(\";\")\n",
    "    word_score_list = []\n",
    "    for item in line_splitted[2:]:\n",
    "        if \"_\" in item:\n",
    "            word, score = item.split(\"_\")\n",
    "            word_score_list.append((word,score))\n",
    "            \n",
    "    return word_score_list\n",
    "\n",
    "def get_Nasari_vectors(query_string):\n",
    "    nasari_vectors = list()\n",
    "    file = open('utils/NASARI_vectors/dd-small-nasari-15.txt', 'r' , encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        if query_string in line:\n",
    "            nasari_vectors.append(vector_format(line))\n",
    "    file.close()\n",
    "    return nasari_vectors\n",
    "\n",
    "#restituisce un dizionario, dove, ad ogni parola (chiave) è associata \n",
    "#una lista di vettori NASARI\n",
    "#MAPPING -APPROCCIO:\n",
    "#associare ad una word il set di vettori NASARI facendo matchare il wikititlepage del vettore\n",
    "#se la ricerca dei vettori avviene con la stringa del tipo ;Word; allora verrà\n",
    "#implementato l'approccio 1, restituendo le righe corrispondenti ai vettori\n",
    "#che contengono quella stringa\n",
    "def get_Nasari_vectors_for_bag_of_words(bag_of_words):\n",
    "    nasari_vectors_for_bag_of_words = dict()\n",
    "    for word in bag_of_words:\n",
    "        query_string = ';' + word.capitalize() + ';' #la ricerca avviene nel secondo approccio\n",
    "        nasari_vectors = get_Nasari_vectors(query_string)\n",
    "        if word not in nasari_vectors_for_bag_of_words.keys() and nasari_vectors:\n",
    "            nasari_vectors_for_bag_of_words[word] = nasari_vectors\n",
    "    return nasari_vectors_for_bag_of_words\n",
    "\n",
    "def get_context_paragraph(paragraph):\n",
    "    return get_Nasari_vectors_for_bag_of_words(bag_of_words(paragraph))\n",
    "\n",
    "#restituisce le chiavi (dimensioni) comuni tra due vettori NASARI\n",
    "def get_common_keys(vector1, vector2):\n",
    "    common_keys = []\n",
    "    for word1,score1 in vector1:\n",
    "        for word2, score2 in vector2:\n",
    "            if word1 == word2:\n",
    "                common_keys.append(word1)\n",
    "    return common_keys\n",
    "\n",
    "#calcola il rango di una chiave (dimensione) all'interno del vettore NASARI in input\n",
    "def rank(key, vector):\n",
    "    for index,(word,value) in enumerate(vector):\n",
    "        if word == key: return index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb985a",
   "metadata": {},
   "source": [
    "FUNZIONI PER IL CALCOLO DEL RANKING DEI PARAGRAFI E CALCOLO DEL MASSIMO WEIGHTED_OVERLAP TRA DUE CONCETTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e50a0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resistuisce il massimo weighted_overlap tra due concetti associati a due parole\n",
    "#i concetti sono liste di vettori, quindi massimizza il weighted_overlap tra due liste di\n",
    "#vettori NASARI\n",
    "def similarity(vector_list1, vector_list2):\n",
    "    max_overlap = 0\n",
    "    \n",
    "    for vector1 in vector_list1:\n",
    "        for vector2 in vector_list2:\n",
    "            overlap = math.sqrt(compute_weighted_overlap(vector1,vector2))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "    return max_overlap\n",
    "\n",
    "#calcola il weighted_overlap tra due vettori NASARI\n",
    "def compute_weighted_overlap(vector1,vector2):\n",
    "    overlap = 0\n",
    "    common_keys = get_common_keys(vector1, vector2)\n",
    "    \n",
    "    if len(common_keys) > 0:\n",
    "        numerator = 0\n",
    "        for q in common_keys:\n",
    "            numerator += (1 / (rank(q, vector1) + rank(q, vector2)))\n",
    "        \n",
    "        denominator = 0\n",
    "        for i in range(1, len(common_keys) + 1):\n",
    "            denominator += 1/ (2 * i)\n",
    "        \n",
    "        overlap = numerator / denominator\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2b911-3ea0-4377-bc45-1ef0371c7a2b",
   "metadata": {},
   "source": [
    "FUNZIONI PER LA VALUTAZIONE DEI RISULTATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9e65da8-2bee-4851-bac8-554a232a2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRECISION e RECALL sui termini più importanti\n",
    "def BLUE_ROUGE_terms_evaluation(document,system_summary, reduction):\n",
    "    gold_important_words = get_important_words(document, reduction)\n",
    "    system_words = get_words(system_summary)\n",
    "\n",
    "    print(\"Document's important words: \\n\", gold_important_words)\n",
    "    print(\"\\nSystem summary words: \\n\", system_words)\n",
    "    \n",
    "    precision = len(gold_important_words.intersection(system_words)) / len(system_words)\n",
    "    recall = len(gold_important_words.intersection(system_words)) / len(gold_important_words)\n",
    "    return precision,recall\n",
    "    \n",
    "#restituisce il dizionario dei tf per ogni parola nel documento\n",
    "#ogni parola avrà tanti tf quanti sono i paragrafi del documento\n",
    "def get_tf_dictionary(document):\n",
    "    tf_dictionary = dict()\n",
    "    for paragraph in document[1:]:\n",
    "        bag_of_words_par = remove_stopwords(tokenize_sentence(remove_punctuation(paragraph)))\n",
    "        tf_par = Counter(bag_of_words_par)\n",
    "        for word in tf_par.keys():\n",
    "            if word not in tf_dictionary.keys(): tf_dictionary[word] = [tf_par[word] / len(bag_of_words_par)]\n",
    "            else: tf_dictionary[word].append(tf_par[word] / len(bag_of_words_par))\n",
    "    return tf_dictionary       \n",
    "            \n",
    "def get_idf_dictionary(document,tf_dictionary):\n",
    "    idf_dictionary = dict()\n",
    "    n_paragraph = len(document[1:])\n",
    "    for word in tf_dictionary.keys():\n",
    "        n_paragraph_contains_word = 0\n",
    "        for paragraph in document[1:]:\n",
    "            if word in bag_of_words(paragraph):\n",
    "                n_paragraph_contains_word += 1\n",
    "        idf_dictionary[word] = math.log(n_paragraph / n_paragraph_contains_word)\n",
    "    return idf_dictionary\n",
    "\n",
    "def get_tf_idf_dictionary(tf_dictionary,idf_dictionary):\n",
    "    tf_idf_dictionary = dict()\n",
    "    for word in tf_dictionary.keys():\n",
    "        tfs_score = tf_dictionary[word] #tutti i term frequency associati alla word\n",
    "        idf_score = idf_dictionary[word] #idf associato alla word\n",
    "        tf_idf_dictionary[word] = mean([tf * idf_score for tf in tfs_score])\n",
    "    return tf_idf_dictionary\n",
    "\n",
    "#restituisce il bag of words di un documento\n",
    "def get_words(document):\n",
    "    bag_of_words_document = set()\n",
    "    for paragraph in document[1:]:\n",
    "        bag_of_words_par = (bag_of_words(paragraph))\n",
    "        bag_of_words_document = bag_of_words_document | bag_of_words_par\n",
    "    return bag_of_words_document\n",
    "\n",
    "#restituisce una lista di coppie (word, tf-idf) relative a document \n",
    "#ordinate secondo il valore di tf-idf        \n",
    "def get_important_words(document, reduction):\n",
    "    #word -> tf1,tf2,tf3,...\n",
    "    #ogni tf è relativo al termine per un paragrafo\n",
    "    #un termine avrà n tf per ogni paragrafo del documento\n",
    "    tf_dictionary = get_tf_dictionary(document)\n",
    "    #word -> idf\n",
    "    #un termine avrà un solo idf\n",
    "    idf_dictionary = get_idf_dictionary(document, tf_dictionary)\n",
    "    \n",
    "    #un termine avrà n tf-idf. verrà preso il tf-idf medio\n",
    "    tf_idf_dictionary = get_tf_idf_dictionary(tf_dictionary, idf_dictionary)\n",
    "    \n",
    "    #calcoliamo il numero di termini da mantenere (saranno quelle più importanti)\n",
    "    #il numero di parole è dato da len(tf_idf_dictionary) * (100 - reduction)/100\n",
    "    percentage = (100 - reduction)/100\n",
    "    important_words_number = int(round(len(tf_idf_dictionary) * percentage))\n",
    "    \n",
    "    #vengono ordinati in modo decrescente gli score tf-idf\n",
    "    sorted_tf_idf = sorted(tf_idf_dictionary.items(), key=lambda x: x[1], reverse=True)[:important_words_number]\n",
    "    \n",
    "    #restituisco solo i termini (senza score)\n",
    "    important_words = set()\n",
    "    for item in sorted_tf_idf: important_words.add(item[0])\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f309a-c83c-4d85-a334-fc9412cc3dbd",
   "metadata": {},
   "source": [
    "FUNZIONE CHE EFFETTUA IL RIASSUNTO DI UN DOCUMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4860793b-b6dd-4bf9-ac4a-1a4f363c5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(document, reduction, relevance_criteria):\n",
    "    \n",
    "    if relevance_criteria == 'title':\n",
    "        topic = get_title_topic(document)\n",
    "    elif relevance_criteria == 'cue':\n",
    "        topic = get_topic(document)\n",
    "    \n",
    "    print()\n",
    "    print(\"TOPIC ESTRATTI: \")\n",
    "    print(topic)\n",
    "    \n",
    "    paragraphs_overlap = []\n",
    "    for paragraph in document[1:]:\n",
    "        paragraph_context = get_context_paragraph(paragraph)\n",
    "        \n",
    "        average_topic_paragraph_overlap = 0 #overlap medio sul pragrafo corrente\n",
    "        match_count = 0 #conteggio totale degli overlap calcolati\n",
    "        for key1 in paragraph_context.keys():\n",
    "            for key2 in topic.keys():\n",
    "                #calcolo e sommo iterativamente la massimizzazione della similarità tra due concetti\n",
    "                #uno individuato dalla chiave nel contesto del paragrafo\n",
    "                #uno individuato dalla chiave nel topic\n",
    "                #ad ogni chiave corrisponde un concetto, individuato come una lista di vettori NASARI\n",
    "                average_topic_paragraph_overlap += similarity(paragraph_context[key1], topic[key2])\n",
    "                match_count += 1\n",
    "        \n",
    "        #calcolo la media per il paragrafo corrente e aggiunto il paragrafo con il suo score\n",
    "        # in una lista di tuple (paragrafo,score)\n",
    "        if match_count != 0:\n",
    "            average_topic_paragraph_overlap = average_topic_paragraph_overlap / match_count\n",
    "            paragraphs_overlap.append((paragraph,average_topic_paragraph_overlap))\n",
    "    \n",
    "    #calcoliamo il numero di paragrafi da manterenere nel riassunto\n",
    "    number_of_paragraphs = len(paragraphs_overlap) - int(round((reduction / 100) * len(paragraphs_overlap), 0))\n",
    "    \n",
    "    #ordiniamo in modo descrescente la lista di tuple (paragrafo, score)\n",
    "    paragraphs_overlap = sorted(paragraphs_overlap, key=lambda x: x[1], reverse = True)[:number_of_paragraphs]\n",
    "                    \n",
    "    #ordiniamo i paragrafi nella lista list_of_paragraphs tenendo conto dell'ordine in cui i paragrafi\n",
    "    #compaiono nel documento originale\n",
    "    summary = []\n",
    "    summary.append(document[0]) #aggiungiamo il titolo come primo paragrafo del riassunto\n",
    "    list_of_paragraphs = [paragraph[0] for paragraph in paragraphs_overlap]\n",
    "    for paragraph in document[1:]: \n",
    "        if paragraph in list_of_paragraphs: \n",
    "            summary.append(paragraph)\n",
    "            \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f0ed4e5-b19b-42fe-879c-64746036c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome del file da analizzare:  Andy-Warhol.txt\n",
      "Nome del file da analizzare:  Ebola-virus-disease.txt\n",
      "Nome del file da analizzare:  Life-indoors.txt\n",
      "Nome del file da analizzare:  Napoleon-wiki.txt\n",
      "Nome del file da analizzare:  Trump-wall.txt\n",
      "\n",
      "PARAGRAFO PIU' IMPORTANTE: \n",
      " ('He anticipated celebrity culture and social media, thought artists should do more than just hold a paintbrush, and wound up John Lennon. As a new Tate exhibition opens, Alastair Smart shows how far the most important artist of the modern age was ahead of his time.', 3)\n",
      "{'age', 'paintbrush', 'social', 'modern', 'celebrity', 'time', 'thought', 'culture', 'hold', 'lennon', 'artist', 'tate', 'alastair', 'ahead', 'open', 'smart', 'medium', 'wind', 'anticipate', 'john', 'exhibition'}\n",
      "\n",
      "TOPIC ESTRATTI: \n",
      "{'social': [[('social', '800.23'), ('sociology', '208.27'), ('theory', '159.87'), ('behavior', '115.39'), ('society', '92.18'), ('marx', '89.36'), ('surplus', '87.87'), ('individual', '76.77'), ('loneliness', '75.56'), ('competence', '73.71'), ('interaction', '72.43'), ('motivation', '65.21'), ('devdas', '52.97'), ('nonmarket', '49.81')]], 'celebrity': [[('celebrity', '1142.71'), ('fame', '189.43'), ('show', '106.31'), ('stavers', '74.53'), ('famous', '64.76'), ('celebrities', '54.74'), ('mixi', '44.7'), ('gaga', '41.44'), ('magazine', '37.35'), ('contestant', '36.63'), ('chef', '36.21'), ('superstar', '35.94'), ('paparazzi', '35.2'), ('testimonial', '32.5')]], 'time': [[('time', '1324.58'), ('clock', '977.93'), ('spacetime', '341.68'), ('universe', '268.25'), ('dst', '265.61'), ('observer', '244.01'), ('event', '175.84'), ('space', '172.51'), ('atomic clock', '128.05'), ('special relativity', '126.23'), ('reference frame', '123.56'), ('future', '122.31'), ('object', '118.33'), ('measure', '115.58')]], 'thought': [[('mind', '734.11'), ('thought', '542.51'), ('human', '265.99'), ('thinking', '258.27'), ('brain', '232.68'), ('consciousness', '215.15'), ('mental', '202.06'), ('theory', '194.35'), ('psychology', '192.92'), ('cognitive', '171.78'), ('idea', '167.57'), ('mental state', '134.94'), ('nous', '131.63'), ('steiner', '120.0')]], 'culture': [[('culture', '2036.8'), ('cultural', '1238.48'), ('sociology', '285.41'), ('anthropology', '263.75'), ('social', '177.74'), ('anthropologist', '173.96'), ('society', '146.18'), ('human', '142.45'), ('study', '134.03'), ('organizational', '131.97'), ('behavior', '114.64'), ('theory', '104.12'), ('individual', '86.33'), ('argue', '83.46')]], 'artist': [[('artist', '714.68'), ('duchamp', '409.98'), ('art', '393.23'), ('audubon', '391.75'), ('hirschfeld', '186.85'), ('tiffany', '156.19'), ('chucky', '118.42'), ('work', '110.64'), ('drawing', '96.15'), ('lear', '94.56'), ('ankomah', '94.24'), ('painting', '86.57'), ('landers', '79.68'), ('magalios', '78.64')]]}\n",
      "\n",
      "RIASSUNTO DEL DOCUMENTO ANALIZZATO:\n",
      "Andy Warhol: Why the great Pop artist thought ‘Trump is sort of cheap’\n",
      "He anticipated celebrity culture and social media, thought artists should do more than just hold a paintbrush, and wound up John Lennon. As a new Tate exhibition opens, Alastair Smart shows how far the most important artist of the modern age was ahead of his time.\n",
      "Reaction on social media was swift, widespread and mostly containing the word “bizarre”. Why show a low-action clip from 1982, of an artist who died in 1987, at a sporting showcase in 2019? What financial sense did it make also – given that advertising slots at the Super Bowl are the most expensive in the world, costing $175,000 a second?\n",
      "“If that clip proves anything, it’s that Andy is still very much with us,” says Gregor Muir, curator of a major Warhol show that opens at Tate Modern this month. “He’s a timeless figure, and our aim with the exhibition is to show that.”\n",
      "After a decade working as a commercial illustrator on Madison Avenue, Warhol began his artistic career at the turn of the 1960s. He sprung to fame as a leader of the Pop Art movement, which rejected high-cultural tradition and depicted everyday subject matter instead: in Warhol’s case, Coke bottles, Brillo boxes and Campbell’s Soup cans.\n",
      "The Tate exhibition sets out to show how large an influence Warhol has had on artists after him, especially in the way he embraced different media and means of distribution. In the mid-1960s, after the early burst of Pop works, he took to making films, such as Sleep, in which his friend, the poet John Giorno, can be seen sleeping for five hours.\n",
      "In short, he believed an artist could, and should, do more than just hold a paintbrush. An interdisciplinary approach to art is today taken as standard – and that’s in no small part thanks to the example set by Warhol’s adventures decades ago.\n",
      "His influence extends far beyond the art world, though. “Warhol both predates and predicts the society we live in,” says Muir. On show at Tate Modern will be 25 paintings from a little-known series of paintings from 1975 called Ladies and Gentlemen. It depicts drag queens and trans women who frequented the Gilded Grape bar, off Times Square – and, in Muir’s view, “could easily pass for having been made yesterday, in terms of the debates being had now around trans identity and LGBT rights”.\n",
      "Though dead for 33 years, Warhol anticipated the advent of social media. Take his Screen Tests, for example, 472 short films in which visitors to his studio were placed on a chair in front of a video camera and asked to “perform” solo for three minutes to camera – very much like the YouTube vloggers of today.\n",
      "Then there’s his 1975 book The Philosophy of Andy Warhol, usually described as a disjointed autobiography but better understood as a proto-Twitter account, given its author’s propensity for including both the most mundane of details (such as his love of jam on toast) and witty one-liners (such as “buying is more American than thinking”).\n",
      "These individual episodes in Warhol’s career show his innate grasp of communications technology; the speed at which it was changing; and the direction in which it was heading.\n",
      "He was aware of the inexorable rise of television, particularly with the proliferation of cable channels such as CNN, ESPN, Nickelodeon and MTV in the late-1970s and early-1980s. Warhol declared at the time that television “was the medium [he’d] most now like to shine in”, and he actually presented five episodes of an MTV talk show called Andy Warhol’s Fifteen Minutes – a job cut short by his death, aged 58, from complications after a gall bladder operation. The show’s title referred to his most famous quote: “In the future, everyone will be world-famous for 15 minutes.”\n",
      "With these words, Warhol predicted the coming both of reality TV stars and social media influencers – as the platforms to achieve quick-fire celebrity grew exponentially.\n",
      "“It’s often said Warhol was the most important artist of the second half of the 20th century,” Muir says. “But one might well argue he’s the most important artist of the early 21st century too.” In a consumerist society, he realised that celebrities are every bit as ubiquitous and disposable as soup cans, Coke bottles and Burger King Whoppers.\n",
      "\n",
      "Document's important words: \n",
      " {'visitor', 'early', 'pop', 'gregor', 'second', 'square', 'bit', 'interview', 'pass', 'month', 'tests', 'modern', 'choose', 'include', 'occasional', 'hour', 'conceive', 'speed', 'slot', 'root', 'star', 'reaction', 'reject', 'feel', 'well', 'reality', 'movement', 'giorno', 'sight', 'predates', 'trump', 'disposable', 'cross', 'grasp', 'easily', 'exponentially', 'burger', 'king', 'woman', 'colour', 'financial', 'footage', 'ahead', 'example', 'prove', 'word', 'book', 'band', 'refuse', 'studio', 'food', 'stage', 'american', 'bowl', 'film', 'coke', 'identity', 'cher', 'john', 'account', 'grape', 'mtv', 'camera', 'distribution', 'ubiquitous', 'television', 'witty', 'treat', 'plastic', 'exploding', 'start', 'propensity', 'elton', 'fame', 'drop', 'great', 'tradition', 'trans', 'career', 'view', 'direction', 'dead', 'diary', 'lowaction', 'realise', 'trumps', 'predict', 'live', 'warhola', 'episode', 'tv', 'forget', 'celebrity', 'burst', 'embrace', 'vloggers', 'perform', 'box', 'philosophy', 'ethnic', 'demise', 'add', 'lgbt', 'argue', 'good', 'hold', 'whopper', 'timeless', 'quickfire', 'prototwitter', 'soup', 'youtube', '20th', 'lennon', 'cheap', 'influencers', 'littleknown', 'wind', 'disjointed', 'times', 'jam', 'context', 'magazine', 'advertise', 'half', 'commission', 'standard', 'didnt', 'decade', 'surname', 'spring', 'avenue', 'leader', 'subject', 'underground', 'detail', 'change', 'brillo', 'autobiography', 'painting', 'break', 'anglicise', 'portrait', 'finished', 'sort', 'gentlemen', 'influence', 'tower', 'pay', '21st', 'mid1960s', 'describe', 'buying', 'path', 'video', 'advert', 'party', 'write', 'technology', 'bar', 'create', 'thought', 'commercial', 'approach', 'small', 'communication', 'century', 'country', 'bite', 'super', 'answer', 'series', 'donald', 'everyday', 'ketchup', 'toast', 'major', 'andrej', 'chair', 'highcultural', 'smart', 'open', 'expensive', 'cent', 'solo', 'sense', 'matter', 'aim', 'head', 'consumerist', 'jackson', 'manage', 'large', 'advent', 'year', 'ladies', 'cost', 'scheme', 'velvet', 'campbells', 'widespread', 'screen', 'michael', 'bizarre', 'oneliners', 'madison', 'unexpected', 'swift', 'yesterday', 'paintbrush', 'social', 'debate', 'frequent', 'society', 'artistic', 'bottle', 'individual', 'art', 'showcase', 'grow', 'front', 'platform', 'love', '1960s', 'sleep', 'sport', 'drag', 'innate', 'culture', 'understood', 'term', 'die', 'illustrator', 'whoppers', 'eat', 'occasion', 'poet', 'interestingly', 'multisensory', 'queen', 'uring', 'alastair', 'interdisciplinary', 'case', 'friend', 'viewer', 'figure', 'inevitable', 'ago', 'clip', 'gilded', 'muir', 'extend', 'muirs', 'curator', 'set', 'mundane', 'short', 'exhibition', 'celebration', 'achieve', 'adventure', 'author'}\n",
      "\n",
      "System summary words: \n",
      " {'visitor', 'early', 'pop', 'gregor', 'second', 'square', 'inexorable', 'worldfamous', 'bit', 'pass', 'month', 'modern', 'tests', 'include', 'hour', 'slot', 'speed', 'star', 'reaction', 'reject', 'well', 'reality', 'movement', 'giorno', 'predates', 'disposable', 'grasp', 'easily', 'exponentially', 'woman', 'king', 'burger', 'financial', 'ahead', 'example', 'call', 'prove', 'refer', 'word', 'book', 'studio', 'american', 'cable', 'bowl', 'film', 'coke', 'identity', 'john', 'account', 'grape', 'mtv', 'camera', 'distribution', 'ubiquitous', 'witty', 'television', 'declare', 'early1980s', 'fame', 'propensity', 'age', 'tradition', 'trans', 'career', 'view', 'direction', 'aware', 'dead', 'lowaction', 'realise', 'predict', 'warhols', 'live', 'place', 'episode', 'celebrity', 'tv', 'burst', 'talk', 'embrace', 'vloggers', 'perform', 'box', 'philosophy', 'lgbt', 'good', 'argue', 'hold', 'work', 'timeless', 'channel', 'quickfire', 'prototwitter', 'soup', 'lennon', 'youtube', '20th', 'gall', 'espn', 'influencers', 'medium', 'littleknown', 'wind', 'times', 'disjointed', 'jam', 'advertise', 'half', 'standard', 'decade', 'spring', 'avenue', 'leader', 'subject', 'detail', 'brillo', 'change', 'autobiography', 'painting', 'gentlemen', 'influence', '21st', 'mid1960s', 'buying', 'describe', 'video', 'technology', 'time', 'bar', 'quote', 'turn', 'thought', 'commercial', 'famous', 'small', 'approach', 'communication', 'series', 'century', 'super', 'everyday', 'artist', 'title', 'nickelodeon', 'toast', 'major', 'highcultural', 'chair', 'open', 'smart', 'expensive', 'matter', 'solo', 'sense', 'shine', 'aim', 'head', 'consumerist', 'future', 'bladder', 'large', 'advent', 'anticipate', 'ladies', 'year', 'cost', 'campbells', 'death', 'widespread', 'screen', 'bizarre', 'madison', 'oneliners', 'yesterday', 'swift', 'fifteen', 'paintbrush', 'social', 'debate', 'proliferation', 'depict', 'frequent', 'society', 'artistic', 'complication', 'bottle', 'individual', 'operation', 'art', 'job', 'showcase', 'warhol', 'grow', 'front', 'love', '1960s', 'platform', 'sleep', 'sport', 'drag', 'culture', 'understood', 'innate', 'term', 'die', 'illustrator', 'late1970s', 'whoppers', 'poet', 'today', 'tate', 'andy', 'queen', 'alastair', 'case', 'friend', 'interdisciplinary', 'figure', 'cut', 'ago', 'clip', 'rise', 'gilded', 'muir', 'extend', 'muirs', 'cnn', 'curator', 'minutes', 'set', 'minute', 'mundane', 'short', 'exhibition', 'achieve', 'adventure', 'author'}\n",
      "\n",
      "Bleu scores:  0.810126582278481\n",
      "Rogue scores:  0.7300380228136882\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    files = Path('utils/docs/').glob('*.txt')\n",
    "    for file in files:\n",
    "        print(\"Nome del file da analizzare: \",file.name)\n",
    "    files.close()\n",
    "    \n",
    "    file_name = input(\"Inserire il nome del file da riassumere (compreso di .txt):\\n\")\n",
    "    reduction = int(input(\"Inserire la percentuale di riduzione (10,20,30):\\n\"))\n",
    "    \n",
    "    files = Path('utils/docs/').glob('*.txt')\n",
    "    document = None\n",
    "    for file in files:\n",
    "        if file.name == file_name:\n",
    "            document = file\n",
    "            summary = summarization(parse_document(file), reduction, relevance_criteria='cue')\n",
    "            print(\"\\nRIASSUNTO DEL DOCUMENTO ANALIZZATO:\")\n",
    "            for par in summary:\n",
    "                print(par)\n",
    "            print()\n",
    "    files.close()\n",
    "\n",
    "    precision,recall = BLUE_ROUGE_terms_evaluation(parse_document(document),summary, reduction)\n",
    "    print()\n",
    "    print(\"Bleu scores: \",precision)\n",
    "    print(\"Rogue scores: \",recall)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
